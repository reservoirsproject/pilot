{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call WOfS using bounding boxes\n",
    "This will be used to get exact extents of reservoirs for the depth-to-surface area relationship. It's an automated way to call the right satellite data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "#import glob    #This one lets you read all the csv files in a directory\n",
    "import rasterio.crs\n",
    "from tqdm.auto import tqdm #this one is a loading bar, it's cool to add loading bars to loops\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask load the satellite data for all the reservoirs\n",
    "I made a shapefile in ArcMAP that has bounding boxes of the reservoirs identified in 00_Library_reservoirs. This is what we will use for the query, so wofs knows where to make the picture for each reservoir. The following code blocks were copied from a DEA notebook called 'Open and run analysis on multiple polygons'. In this case the multiple polygons are my bounding boxes from the geodataframe above. First you make a query with no x, y points and no CRS. Just the time. Then you loop the location for the query using a datacube package called geomoetry. Put the dc.load() line in the loop (I'm going to dask load, not load actual images, I'll do that later after I've merged with the gauges). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('00_Lib_bound/00_Lib_bound.shp')\n",
    "\n",
    "query = {'time': ('01-01-1988', '09-12-2020')} \n",
    "         #'crs': 'EPSG:3577'}\n",
    "dc = datacube.Datacube(app='dc-WOfS')\n",
    "\n",
    "results = {} \n",
    "\n",
    "#tqdm is gonna make the bar. tqdm is Arabic abbreviation for 'progress'\n",
    "for index, row in tqdm(gdf.iterrows(), total=len(gdf)):\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    query.update({'geopolygon': geom})\n",
    "    \n",
    "    wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, group_by='solar_day', **query)\n",
    "    \n",
    "    poly_mask = xr_rasterize(gdf.iloc[[index]], wofs_albers)\n",
    "    wofs_albers = wofs_albers.where(poly_mask, other=wofs_albers.water.nodata) #put other or all the data turns into 0\n",
    "    \n",
    "    results.update({str(row['gauge_ID']): wofs_albers}) #The handle for dictionary objects is the gauge ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop read all the csv files in 00_Library\n",
    "Now we have a library of the wofs data with the gauge ID as the key. We now need a library of the depth data with, again, the gauge ID as the key. Then we can match them up later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of the file names so we can call them with pandas\n",
    "file_list = []\n",
    "\n",
    "directory = '00_Library'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_list.append(os.path.join(directory, filename))\n",
    "\n",
    "#Read the gauge files twice, once to get ID and second to get the data. Append them together in a dictionary\n",
    "#May as well make a list of IDs here because we will probably use it later\n",
    "data_dict = {}        \n",
    "ID_list = []\n",
    "#let's use tqdm again to make a progress bar. The bar is so cool I love this module\n",
    "#I'm gonna use tqdm on literally all of my loops now\n",
    "for i in tqdm(file_list, total=len(file_list)):\n",
    "    df = pd.read_csv(i, nrows=1, escapechar='#')\n",
    "    column = df.iloc[:,[1]] #This is the column with the ID in it\n",
    "    ID = list(column)\n",
    "    ID = ID[0]\n",
    "    ID = df.at[0, ID]\n",
    "    ID_list.append(str(ID))\n",
    "    \n",
    "    data = pd.read_csv(i, error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], \n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0]))\n",
    "    data = data.drop(columns=['Quality Code', 'Interpolation Type'])\n",
    "    data_dict.update({str(ID): data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a function that generates the depth slices of a reservoir and gets the depth to surface area relationship\n",
    "This is my first time writing a function, Matthew and Bex from DEA helped me write it. I don't want it to print all the stuff out at the end, I just want it to do the tqdm bar, how do I make it stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prod(gauge_data, wofs_albers, make_plots = False) -> 'depth slices': \n",
    "    \"\"\"\n",
    "    This function takes the gauge data and the wofs data,\n",
    "    cloud masks the images and counts the pixels in each depth slice.\n",
    "    It returns a list of all the surface areas per depth.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Get the depth range and intervals\n",
    "    gauge_data = gauge_data.dropna()\n",
    "    depth_integers = gauge_data.astype(np.int64)\n",
    "    max_depth = depth_integers.Value.max()\n",
    "    min_depth = depth_integers.Value.min()\n",
    "    integer_array = depth_integers.Value.unique()\n",
    "    integer_list = integer_array.tolist()\n",
    "    \n",
    "    gauge_data_xr = gauge_data.to_xarray() #convert gauge data to xarray\n",
    "    merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time) #use xarrays .interp() function to merge\n",
    "\n",
    "    surface_area_list = []\n",
    "\n",
    "    for i in tqdm(integer_list, leave = False):\n",
    "        specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                        (merged_data.Value < i+1), drop=True)\n",
    "        date_list = specified_level.time.values[:20] #caps images at 20 per slice (way faster)\n",
    "        specified_passes = wofs_albers.sel(time=date_list).compute() #This .compute() Xarray function loads actual images\n",
    "        #cloudmask (Claire Krause wrote this for me)\n",
    "        #print(specified_passes.water)\n",
    "        cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "        ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "        # Calculate the total number of pixels per timestep\n",
    "        npixels_per_slice = (specified_passes.water.shape[1] * \n",
    "                             specified_passes.water.shape[2])\n",
    "        cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "        clear_specified_passes = specified_passes.water.isel(\n",
    "            time=cloud_pixels_fraction < 0.2)\n",
    "        wet = masking.make_mask(clear_specified_passes, wet=True).sum(dim='time')\n",
    "        dry = masking.make_mask(clear_specified_passes, dry=True).sum(dim='time')\n",
    "        clear = wet + dry\n",
    "        frequency = wet / clear\n",
    "        frequency = frequency.fillna(0)  \n",
    "\n",
    "        #Get area from the satellite data\n",
    "        #get the frequency array\n",
    "        frequency_array = frequency.values\n",
    "        #Turn any pixel in the frequency array with a value greater than 0.2 into a pixel of value 1\n",
    "        #if the pixel value is 0.2 or lower it gets value 0\n",
    "        is_water = np.where((frequency_array > 0.2),1,0)\n",
    "        #give the 'frequency' xarray back its new values of zero and one\n",
    "        frequency.values = is_water\n",
    "        #sum up the pixels\n",
    "        number_water_pixels = frequency.sum(dim=['x', 'y'])\n",
    "        #get the number\n",
    "        number_water_pixels = number_water_pixels.values.tolist()\n",
    "        #multiply by pixel size to get area in m2\n",
    "        area_m2 = number_water_pixels*(25*25)\n",
    "\n",
    "        surface_area_list.append(area_m2)\n",
    "        #print('This is the area as calculated from wet pixels at', i, 'meters', area_m2)\n",
    "\n",
    "        #Plotting the image\n",
    "        if make_plots:\n",
    "            frequency.plot(figsize = (7,5))\n",
    "        \n",
    "    return surface_area_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the function for all of the reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in tqdm(ID_list, total=len(ID_list)):\n",
    "    if (ID in data_dict.keys()) and (ID in results.keys()):\n",
    "        image_prod(data_dict[ID], results[ID], make_plots = False)\n",
    "    else:\n",
    "        print('we didnt find', ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a function that generates the depth to surface area lookup table and outputs monthly surface area graphs\n",
    "Now we have all the slices we can make a look-up table that has 4 columns: depth, surface area, reservoir name, gauge ID. And I just want it to be one big table, not a bunch of small tables. And then I can save it as a csv file and then open in excel and ask the other team what they want to do with it now. Also I want the output of the function to be the monthly surface area hydrograph for each reservoir as it loops over each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_maker()\n",
    "    orig_hydrograph = pd.read_csv(csv,\n",
    "                    error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                             parse_dates=['Timestamp'], #Tells it this column is date format\n",
    "                             index_col=('Timestamp'), #Tells it to set Timestamp as the index column\n",
    "                            date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0])) #turns timestamp into date\n",
    "    orig_hydrograph = orig_hydrograph.drop(columns=['Interpolation Type', 'Quality Code'])\n",
    "    orig_hydrograph.plot(figsize=(16, 5))\n",
    "    print('This is the original hydrograph: depth over time')\n",
    "\n",
    "    #create dataframe of depth to surface area \n",
    "    depth_to_area_df = DataFrame(integer_list, columns=['Depth'])\n",
    "    depth_to_area_df['Surface Area'] = new_surface_area_list\n",
    "    depth_to_area_df['Name'] = name\n",
    "    depth_to_area_df['ID'] = ID_str\n",
    "    depth_to_area_df\n",
    "\n",
    "    orig_hydrograph = orig_hydrograph.dropna()\n",
    "\n",
    "    hydrograph1 = pd.read_csv(csv,\n",
    "                    error_bad_lines = False, skiprows=9, escapechar='#', parse_dates=['Timestamp'])\n",
    "    hydrograph1 = hydrograph1.drop(columns=['Interpolation Type', 'Quality Code'])\n",
    "    hydrograph1 = hydrograph1.dropna()\n",
    "\n",
    "    depth_integers = hydrograph1.Value.astype(np.int64)\n",
    "    depth_integers_list = depth_integers.to_list()\n",
    "\n",
    "    orig_hydrograph['Depth'] = depth_integers_list\n",
    "    orig_hydrograph['Date'] = orig_hydrograph.index\n",
    "    orig_hydrograph\n",
    "\n",
    "    merged = (orig_hydrograph\n",
    "              .merge(depth_to_area_df[['Surface Area', 'Depth', 'Name', 'ID']], on='Depth'))\n",
    "    df1 = merged.sort_values(['Date'])\n",
    "    #pd.read_csv('surface_area_timeseries.csv', parse_dates=['Date'])\n",
    "    df = df1.set_index(['Date'])\n",
    "    #Use this pandas function MS (monthly summary) to \n",
    "    df = df.resample('MS').sum()\n",
    "    df = df.drop(columns = ['Value', 'Depth'])\n",
    "    df['Name'] = df1.at[0, 'Name']\n",
    "    df['ID'] = df1.at[0, 'ID']\n",
    "    df.plot(figsize=(16, 5))\n",
    "    \n",
    "return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
