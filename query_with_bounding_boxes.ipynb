{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call WOfS using bounding boxes\n",
    "This will be used to get exact extents of reservoirs for the depth-to-surface area relationship. It's an automated way to call the right satellite data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/cligj/__init__.py:17: FutureWarning: cligj 1.0.0 will require Python >= 3.7\n",
      "  warn(\"cligj 1.0.0 will require Python >= 3.7\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob    #This one lets you read all the csv files in a directory\n",
    "import rasterio.crs\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the bounding box shapefile\n",
    "I made a shapefile in ArcMAP that has bounding boxes of the reservoirs identified in 00_Library_reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>staion_nam</th>\n",
       "      <th>ORIG_FID</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TAYLORS</td>\n",
       "      <td>LAKE TAYLOR</td>\n",
       "      <td>Taylors Lake</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((142.36410 -36.82037, 142.37857 -36.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE604</td>\n",
       "      <td>UPPER STONY CREEK RESERVOIR</td>\n",
       "      <td>Upper Stony</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((144.19442 -37.81257, 144.21163 -37.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sp-o10334</td>\n",
       "      <td>LAKE EILDON</td>\n",
       "      <td>EILDON</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((145.86701 -36.93337, 146.21666 -37.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>425022</td>\n",
       "      <td>LAKE MENINDEE</td>\n",
       "      <td>LAKE MENINDEE</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((142.29594 -32.24831, 142.42359 -32.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sp-o11534</td>\n",
       "      <td>WARANGA BASIN</td>\n",
       "      <td>WARANGA BASIN</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((145.02963 -36.55203, 145.11966 -36.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>136023A</td>\n",
       "      <td>NED CHURCHWARD WEIR</td>\n",
       "      <td>Ned Churchward HW</td>\n",
       "      <td>148</td>\n",
       "      <td>POLYGON ((151.94635 -25.14140, 152.05044 -25.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>136020A</td>\n",
       "      <td>BEN ANDERSON BARRAGE</td>\n",
       "      <td>Ben Anderson Barrage</td>\n",
       "      <td>149</td>\n",
       "      <td>POLYGON ((152.14677 -24.97170, 152.26926 -24.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>136003C</td>\n",
       "      <td>CLAUDE WHARTON WEIR</td>\n",
       "      <td>Claude Wharton HW</td>\n",
       "      <td>150</td>\n",
       "      <td>POLYGON ((151.52403 -25.61861, 151.59165 -25.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>125008A</td>\n",
       "      <td>MARIAN WEIR</td>\n",
       "      <td>Mirani Weir HW</td>\n",
       "      <td>151</td>\n",
       "      <td>POLYGON ((148.82252 -21.15658, 148.92972 -21.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>125008A</td>\n",
       "      <td>MIRANI WEIR</td>\n",
       "      <td>Mirani Weir HW</td>\n",
       "      <td>152</td>\n",
       "      <td>POLYGON ((148.80675 -21.23068, 148.82208 -21.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gauge_ID                         NAME            staion_nam  ORIG_FID  \\\n",
       "0      TAYLORS                  LAKE TAYLOR          Taylors Lake         0   \n",
       "1        RE604  UPPER STONY CREEK RESERVOIR           Upper Stony         1   \n",
       "2    sp-o10334                  LAKE EILDON                EILDON         2   \n",
       "3       425022                LAKE MENINDEE         LAKE MENINDEE         3   \n",
       "4    sp-o11534                WARANGA BASIN         WARANGA BASIN         4   \n",
       "..         ...                          ...                   ...       ...   \n",
       "148    136023A          NED CHURCHWARD WEIR     Ned Churchward HW       148   \n",
       "149    136020A         BEN ANDERSON BARRAGE  Ben Anderson Barrage       149   \n",
       "150    136003C          CLAUDE WHARTON WEIR     Claude Wharton HW       150   \n",
       "151    125008A                  MARIAN WEIR        Mirani Weir HW       151   \n",
       "152    125008A                  MIRANI WEIR        Mirani Weir HW       152   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((142.36410 -36.82037, 142.37857 -36.7...  \n",
       "1    POLYGON ((144.19442 -37.81257, 144.21163 -37.8...  \n",
       "2    POLYGON ((145.86701 -36.93337, 146.21666 -37.1...  \n",
       "3    POLYGON ((142.29594 -32.24831, 142.42359 -32.3...  \n",
       "4    POLYGON ((145.02963 -36.55203, 145.11966 -36.4...  \n",
       "..                                                 ...  \n",
       "148  POLYGON ((151.94635 -25.14140, 152.05044 -25.0...  \n",
       "149  POLYGON ((152.14677 -24.97170, 152.26926 -24.8...  \n",
       "150  POLYGON ((151.52403 -25.61861, 151.59165 -25.6...  \n",
       "151  POLYGON ((148.82252 -21.15658, 148.92972 -21.1...  \n",
       "152  POLYGON ((148.80675 -21.23068, 148.82208 -21.1...  \n",
       "\n",
       "[153 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = gpd.read_file('00_Lib_bound/00_Lib_bound.shp')\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask load the satellite data for all the reservoirs\n",
    "The following code blocks were copied from a DEA notebook called 'Open and run analysis on multiple polygons'. In this case the multiple polygons are my bounding boxes from the geodataframe above. First you make a query with no x, y points and no CRS. Just the time. Then you loop the location for the query using a datacube package called geomoetry. Put the dc.load() line in the loop (I'm going to dask load, not load actual images, I'll do that later after I've merged with the gauges). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 1/153\n",
      "Feature: 2/153\n",
      "Feature: 3/153\n",
      "Feature: 4/153\n",
      "Feature: 5/153\n",
      "Feature: 6/153\n",
      "Feature: 7/153\n",
      "Feature: 8/153\n",
      "Feature: 9/153\n",
      "Feature: 10/153\n",
      "Feature: 11/153\n",
      "Feature: 12/153\n",
      "Feature: 13/153\n",
      "Feature: 14/153\n",
      "Feature: 15/153\n",
      "Feature: 16/153\n",
      "Feature: 17/153\n",
      "Feature: 18/153\n",
      "Feature: 19/153\n",
      "Feature: 20/153\n",
      "Feature: 21/153\n",
      "Feature: 22/153\n",
      "Feature: 23/153\n",
      "Feature: 24/153\n",
      "Feature: 25/153\n",
      "Feature: 26/153\n",
      "Feature: 27/153\n",
      "Feature: 28/153\n",
      "Feature: 29/153\n",
      "Feature: 30/153\n",
      "Feature: 31/153\n",
      "Feature: 32/153\n",
      "Feature: 33/153\n",
      "Feature: 34/153\n",
      "Feature: 35/153\n",
      "Feature: 36/153\n",
      "Feature: 37/153\n",
      "Feature: 38/153\n",
      "Feature: 39/153\n",
      "Feature: 40/153\n",
      "Feature: 41/153\n",
      "Feature: 42/153\n",
      "Feature: 43/153\n",
      "Feature: 44/153\n",
      "Feature: 45/153\n",
      "Feature: 46/153\n",
      "Feature: 47/153\n",
      "Feature: 48/153\n",
      "Feature: 49/153\n",
      "Feature: 50/153\n",
      "Feature: 51/153\n",
      "Feature: 52/153\n",
      "Feature: 53/153\n",
      "Feature: 54/153\n",
      "Feature: 55/153\n",
      "Feature: 56/153\n",
      "Feature: 57/153\n",
      "Feature: 58/153\n",
      "Feature: 59/153\n",
      "Feature: 60/153\n",
      "Feature: 61/153\n",
      "Feature: 62/153\n",
      "Feature: 63/153\n",
      "Feature: 64/153\n",
      "Feature: 65/153\n",
      "Feature: 66/153\n",
      "Feature: 67/153\n",
      "Feature: 68/153\n",
      "Feature: 69/153\n",
      "Feature: 70/153\n",
      "Feature: 71/153\n",
      "Feature: 72/153\n",
      "Feature: 73/153\n",
      "Feature: 74/153\n",
      "Feature: 75/153\n",
      "Feature: 76/153\n",
      "Feature: 77/153\n",
      "Feature: 78/153\n",
      "Feature: 79/153\n",
      "Feature: 80/153\n",
      "Feature: 81/153\n",
      "Feature: 82/153\n",
      "Feature: 83/153\n",
      "Feature: 84/153\n",
      "Feature: 85/153\n",
      "Feature: 86/153\n",
      "Feature: 87/153\n",
      "Feature: 88/153\n",
      "Feature: 89/153\n",
      "Feature: 90/153\n",
      "Feature: 91/153\n",
      "Feature: 92/153\n",
      "Feature: 93/153\n",
      "Feature: 94/153\n",
      "Feature: 95/153\n",
      "Feature: 96/153\n",
      "Feature: 97/153\n",
      "Feature: 98/153\n",
      "Feature: 99/153\n",
      "Feature: 100/153\n",
      "Feature: 101/153\n",
      "Feature: 102/153\n",
      "Feature: 103/153\n",
      "Feature: 104/153\n",
      "Feature: 105/153\n",
      "Feature: 106/153\n",
      "Feature: 107/153\n",
      "Feature: 108/153\n",
      "Feature: 109/153\n",
      "Feature: 110/153\n",
      "Feature: 111/153\n",
      "Feature: 112/153\n",
      "Feature: 113/153\n",
      "Feature: 114/153\n",
      "Feature: 115/153\n",
      "Feature: 116/153\n",
      "Feature: 117/153\n",
      "Feature: 118/153\n",
      "Feature: 119/153\n",
      "Feature: 120/153\n",
      "Feature: 121/153\n",
      "Feature: 122/153\n",
      "Feature: 123/153\n",
      "Feature: 124/153\n",
      "Feature: 125/153\n",
      "Feature: 126/153\n",
      "Feature: 127/153\n",
      "Feature: 128/153\n",
      "Feature: 129/153\n",
      "Feature: 130/153\n",
      "Feature: 131/153\n",
      "Feature: 132/153\n",
      "Feature: 133/153\n",
      "Feature: 134/153\n",
      "Feature: 135/153\n",
      "Feature: 136/153\n",
      "Feature: 137/153\n",
      "Feature: 138/153\n",
      "Feature: 139/153\n",
      "Feature: 140/153\n",
      "Feature: 141/153\n",
      "Feature: 142/153\n",
      "Feature: 143/153\n",
      "Feature: 144/153\n",
      "Feature: 145/153\n",
      "Feature: 146/153\n",
      "Feature: 147/153\n",
      "Feature: 148/153\n",
      "Feature: 149/153\n",
      "Feature: 150/153\n",
      "Feature: 151/153\n",
      "Feature: 152/153\n",
      "Feature: 153/153\n"
     ]
    }
   ],
   "source": [
    "query = {'time': ('01-01-1988', '09-12-2020')} \n",
    "         #'crs': 'EPSG:3577'}\n",
    "dc = datacube.Datacube(app='dc-WOfS')\n",
    "\n",
    "results = {} \n",
    "\n",
    "for index, row in gdf.iterrows():\n",
    "    print(f'Feature: {index + 1}/{len(gdf)}')\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    query.update({'geopolygon': geom})\n",
    "    \n",
    "    wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, group_by='solar_day', **query)\n",
    "    \n",
    "    poly_mask = xr_rasterize(gdf.iloc[[index]], wofs_albers)\n",
    "    wofs_albers = wofs_albers.where(poly_mask)\n",
    "    \n",
    "    results.update({str(row['gauge_ID']): wofs_albers}) #The handle for dictionary objects is the gauge ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop read all the csv files in 00_Library\n",
    "Now we have a library of the wofs data with the gauge ID as the key. We now need a library of the depth data with, again, the gauge ID as the key. Then we can match them up later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of the file names so we can call them with pandas\n",
    "file_list = []\n",
    "\n",
    "directory = '00_Library'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_list.append(os.path.join(directory, filename))\n",
    "\n",
    "#Read the gauge files twice, once to get ID and second to get the data. Append them together in a dictionary\n",
    "#May as well make a list of IDs here because we will probably use it later\n",
    "data_dict = {}        \n",
    "ID_list = []\n",
    "for i in file_list:\n",
    "    df = pd.read_csv(i, nrows=1, escapechar='#')\n",
    "    column = df.iloc[:,[1]] #This is the column with the ID in it\n",
    "    ID = list(column)\n",
    "    ID = ID[0]\n",
    "    ID = df.at[0, ID]\n",
    "    ID_list.append(ID)\n",
    "    \n",
    "    data = pd.read_csv(i, error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], \n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0]))\n",
    "    data_dict.update({str(ID): data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('130304B',\n",
       " <xarray.Dataset>\n",
       " Dimensions:      (time: 789, x: 56, y: 152)\n",
       " Coordinates:\n",
       "   * time         (time) datetime64[ns] 1988-01-06T23:28:45.500000 ... 2019-07...\n",
       "   * y            (y) float64 -2.734e+06 -2.734e+06 ... -2.738e+06 -2.738e+06\n",
       "   * x            (x) float64 1.784e+06 1.784e+06 ... 1.785e+06 1.785e+06\n",
       "     spatial_ref  int32 3577\n",
       " Data variables:\n",
       "     water        (time, y, x) float64 dask.array<chunksize=(1, 152, 56), meta=np.ndarray>\n",
       " Attributes:\n",
       "     crs:           EPSG:3577\n",
       "     grid_mapping:  spatial_ref)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(list(results.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('410131',\n",
       "               Value  Quality Code  Interpolation Type\n",
       " Timestamp                                            \n",
       " 2000-01-01  350.772            90                 603\n",
       " 2000-01-02  350.891            90                 603\n",
       " 2000-01-03  350.999            90                 603\n",
       " 2000-01-04  351.072            90                 603\n",
       " 2000-01-05  351.060            90                 603\n",
       " ...             ...           ...                 ...\n",
       " 2020-11-11  360.649           140                 603\n",
       " 2020-11-12  360.574           140                 603\n",
       " 2020-11-13  360.511           140                 603\n",
       " 2020-11-14  360.437           140                 603\n",
       " 2020-11-15  360.377           140                 603\n",
       " \n",
       " [7625 rows x 3 columns])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(data_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge the wofs arrays with the gauge data for each reservoir\n",
    "We have a library of wofs data and a library of gauge data. In both cases, the key is the gauge ID. I've merged depth dataframes with wofs xarrays before but I don't know how to loop merge them using 2 dictionaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
