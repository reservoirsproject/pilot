{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join satellite image with another dataset using dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/cligj/__init__.py:17: FutureWarning: cligj 1.0.0 will require Python >= 3.7\n",
      "  warn(\"cligj 1.0.0 will require Python >= 3.7\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "#import glob    #This one lets you read all the csv files in a directory\n",
    "import rasterio.crs\n",
    "from tqdm.auto import tqdm #this one is a loading bar, it's cool to add loading bars to loops\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dictionary of reservoir images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block was copied from a DEA notebook called 'Open and run analysis on multiple polygons'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('00_Lib_bound/00_Lib_bound.shp') #The polygons of the extent I want\n",
    "\n",
    "query = {'time': ('01-01-1988', '09-12-2020')} \n",
    "         #'crs': 'EPSG:3577'}\n",
    "dc = datacube.Datacube(app='dc-WOfS')\n",
    "\n",
    "wofs_dict = {} \n",
    "\n",
    "for index, row in gdf.iterrows():\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    query.update({'geopolygon': geom})\n",
    "    \n",
    "    wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, group_by='solar_day', **query)\n",
    "    \n",
    "    poly_mask = xr_rasterize(gdf.iloc[[index]], wofs_albers)\n",
    "    wofs_albers = wofs_albers.where(poly_mask, other=wofs_albers.water.nodata) #put other or all the data turns into 0\n",
    "    \n",
    "    wofs_dict.update({str(row['gauge_ID']): wofs_albers}) #The key for dictionary objects is the gauge ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dictionary of gauge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [] #iteratively read over all the files in a directory\n",
    "\n",
    "directory = '00_Library'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_list.append(os.path.join(directory, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}        \n",
    "ID_list = [] #This list of IDs will come in handy later (list of dictionary keys)\n",
    "\n",
    "for i in file_list:\n",
    "    #get the ID from the gauge data file so we can make it the key\n",
    "    df = pd.read_csv(i, nrows=1, escapechar='#')\n",
    "    column = df.iloc[:,[1]] #This is the column with the ID in it\n",
    "    ID = list(column)\n",
    "    ID = ID[0]\n",
    "    ID = df.at[0, ID]\n",
    "    ID_list.append(str(ID))\n",
    "    \n",
    "    #get the actual gauge data\n",
    "    data = pd.read_csv(i, error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], \n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0]))\n",
    "    data = data.drop(columns=['Quality Code', 'Interpolation Type'])\n",
    "    \n",
    "    data_dict.update({str(ID): data}) #again, the key is the gauge ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## have a look at the dictionaries we want to join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID = 604.1 \n",
      "\n",
      " wofs data: \n",
      " <xarray.Dataset>\n",
      "Dimensions:      (time: 667, x: 605, y: 588)\n",
      "Coordinates:\n",
      "  * time         (time) datetime64[ns] 1988-01-15T23:27:33.500000 ... 2019-07...\n",
      "  * y            (y) float64 -4.587e+06 -4.587e+06 ... -4.602e+06 -4.602e+06\n",
      "  * x            (x) float64 1.204e+06 1.204e+06 ... 1.219e+06 1.219e+06\n",
      "    spatial_ref  int32 3577\n",
      "Data variables:\n",
      "    water        (time, y, x) int16 dask.array<chunksize=(1, 588, 605), meta=np.ndarray>\n",
      "Attributes:\n",
      "    crs:           EPSG:3577\n",
      "    grid_mapping:  spatial_ref \n",
      " \n",
      " \n",
      " \n",
      " gauge data: \n",
      "               Value\n",
      "Timestamp          \n",
      "2000-01-01  121.532\n",
      "2000-01-02  121.540\n",
      "2000-01-03  121.565\n",
      "2000-01-04  121.556\n",
      "2000-01-05  121.553\n",
      "...             ...\n",
      "2020-11-11  120.085\n",
      "2020-11-12  120.174\n",
      "2020-11-13  120.195\n",
      "2020-11-14  120.208\n",
      "2020-11-15      NaN\n",
      "\n",
      "[7625 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "test_ID = ID_list[1]\n",
    "print('ID =', test_ID, '\\n\\n', 'wofs data: \\n', wofs_dict[test_ID], '\\n \\n \\n \\n', 'gauge data: \\n', data_dict[test_ID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append one dictionary to the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "new_dict = defaultdict(list)\n",
    "\n",
    "for i in (data_dict, wofs_dict): #add as many dictionaries with the same key as you want\n",
    "    for key, value in i.items():\n",
    "        new_dict[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[              Value\n",
       " Timestamp          \n",
       " 2000-01-01  121.532\n",
       " 2000-01-02  121.540\n",
       " 2000-01-03  121.565\n",
       " 2000-01-04  121.556\n",
       " 2000-01-05  121.553\n",
       " ...             ...\n",
       " 2020-11-11  120.085\n",
       " 2020-11-12  120.174\n",
       " 2020-11-13  120.195\n",
       " 2020-11-14  120.208\n",
       " 2020-11-15      NaN\n",
       " \n",
       " [7625 rows x 1 columns],\n",
       " <xarray.Dataset>\n",
       " Dimensions:      (time: 667, x: 605, y: 588)\n",
       " Coordinates:\n",
       "   * time         (time) datetime64[ns] 1988-01-15T23:27:33.500000 ... 2019-07...\n",
       "   * y            (y) float64 -4.587e+06 -4.587e+06 ... -4.602e+06 -4.602e+06\n",
       "   * x            (x) float64 1.204e+06 1.204e+06 ... 1.219e+06 1.219e+06\n",
       "     spatial_ref  int32 3577\n",
       " Data variables:\n",
       "     water        (time, y, x) int16 dask.array<chunksize=(1, 588, 605), meta=np.ndarray>\n",
       " Attributes:\n",
       "     crs:           EPSG:3577\n",
       "     grid_mapping:  spatial_ref]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict[test_ID]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
